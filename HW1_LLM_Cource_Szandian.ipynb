{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szandian/LLM/blob/main/HW1_LLM_Cource_Szandian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EE 690/EE 790 Large Language Models\n",
        "\n",
        "Assignment #1 (Tokenizer)\n",
        "\n",
        "Blazer ID: Szandian\n",
        "\n",
        "PhD Student: Somayeh Zandian\n"
      ],
      "metadata": {
        "id": "Ti3xKnC3yecc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt0fNXxm0-Ba",
        "outputId": "53535ed4-5df6-4d82-f47b-a4653756c7d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Tokenize and Decode Sentences (30 points)\n",
        "Write Python code that does the following:\n",
        "- Imports the tiktoken library and loads the GPT-2 tokenizer.\n",
        "- Takes three sentences as input (hard-coded is fine).\n",
        "- Tokenizes each sentence into token IDs using tokenizer.encode().\n",
        "- Decodes the token IDs back into text using tokenizer.decode().\n",
        "- Print: Original sentences, Token ID sequences, Decoded text from token IDs"
      ],
      "metadata": {
        "id": "fjhRrb4ijKiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Bioinformatics combines biology, computer science, and statistics.\",\n",
        "    \"Tokenization is the first step in many NLP pipelines.\"\n",
        "]\n",
        "\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    token_ids = tokenizer.encode(sent)\n",
        "    decoded = tokenizer.decode(token_ids)\n",
        "    print(f\"Sentence {i}: {sent!r}\")\n",
        "    print(f\"  Token IDs: {token_ids}\")\n",
        "    print(f\"  Decoded : {decoded!r}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzJQFeF5jDEY",
        "outputId": "f98214b2-4947-4c47-c479-bd91624eaac9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: 'The quick brown fox jumps over the lazy dog.'\n",
            "  Token IDs: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
            "  Decoded : 'The quick brown fox jumps over the lazy dog.'\n",
            "\n",
            "Sentence 2: 'Bioinformatics combines biology, computer science, and statistics.'\n",
            "  Token IDs: [42787, 259, 18982, 873, 21001, 17219, 11, 3644, 3783, 11, 290, 7869, 13]\n",
            "  Decoded : 'Bioinformatics combines biology, computer science, and statistics.'\n",
            "\n",
            "Sentence 3: 'Tokenization is the first step in many NLP pipelines.'\n",
            "  Token IDs: [30642, 1634, 318, 262, 717, 2239, 287, 867, 399, 19930, 31108, 13]\n",
            "  Decoded : 'Tokenization is the first step in many NLP pipelines.'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Analyze Token Behavior (20 points)\n",
        "- Use the tokenizer to encode the following words:\n",
        "\"hello\", \"Hello\", \"HELLO\", \"hElLo\"\n",
        "- Print the token ID sequences for each.\n",
        "- Use decode() to convert the token IDs back to strings.\n",
        "- Reflection Questions (to answer in comments or a separate text file):\n",
        ". How does the tokenizer treat different capitalizations?\n",
        ". Why do you think subword tokenization is used instead of word-level?"
      ],
      "metadata": {
        "id": "F8leBhFMk2DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"hello\", \"Hello\", \"HELLO\", \"hElLo\"]\n",
        "\n",
        "print(\"=== Part (b): Token Behavior ===\\n\")\n",
        "for w in words:\n",
        "    ids = tokenizer.encode(w)\n",
        "    back = tokenizer.decode(ids)\n",
        "    print(f\"Word {w!r}:\")\n",
        "    print(f\"  Token IDs: {ids}\")\n",
        "    print(f\"  Decoded : {back!r}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1saGLCEm_W1",
        "outputId": "b1a56f4d-0412-4fbe-8b26-7642be887def"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Part (b): Token Behavior ===\n",
            "\n",
            "Word 'hello':\n",
            "  Token IDs: [31373]\n",
            "  Decoded : 'hello'\n",
            "\n",
            "Word 'Hello':\n",
            "  Token IDs: [15496]\n",
            "  Decoded : 'Hello'\n",
            "\n",
            "Word 'HELLO':\n",
            "  Token IDs: [13909, 3069, 46]\n",
            "  Decoded : 'HELLO'\n",
            "\n",
            "Word 'hElLo':\n",
            "  Token IDs: [71, 9527, 27654]\n",
            "  Decoded : 'hElLo'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: How does the tokenizer treat different capitalizations?**\n",
        "\n",
        "A: The tokenizer is case-sensitive: different capitalizations produce different token IDs.\n",
        "\n",
        " **Q:  Why do you think subword tokenization is used instead of word-level?**\n",
        "\n",
        " A: Subword tokenization is used so that rare or unseen words can be broken into known pieces,\n",
        "improving handling of out-of-vocabulary words and reducing total vocabulary size.\n",
        "\n"
      ],
      "metadata": {
        "id": "hvbUDo37oDio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Vocabulary and Unknown Tokens (20 points)\n",
        "- Print the total vocabulary size using:\n",
        "- Try encoding a nonsense word like \"zqxjklmno\" and print its token ID sequence and\n",
        "decoded form.\n",
        "\n",
        "- How many subword units is it broken into?\n",
        "- What does this say about how tiktoken handles unknown or rare words?"
      ],
      "metadata": {
        "id": "UQu6--s-pSk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Total vocabulary size\n",
        "try:\n",
        "    vocab_size = tokenizer.n_vocab\n",
        "except AttributeError:\n",
        "    # Some versions use .vocab_size or len(tokenizer)\n",
        "    vocab_size = len(tokenizer._tokenizer.get_vocab())\n",
        "print(f\"Total GPT-2 vocab size: {vocab_size}\\n\")\n",
        "\n",
        "# 2. Nonsense word\n",
        "nonsense = \"zqxjklmno\"\n",
        "n_ids = tokenizer.encode(nonsense)\n",
        "n_decoded = tokenizer.decode(n_ids)\n",
        "print(f\"Nonsense word {nonsense!r}:\")\n",
        "print(f\"  Token IDs      : {n_ids}\")\n",
        "print(f\"  Decoded        : {n_decoded!r}\")\n",
        "print(f\"  # subword units: {len(n_ids)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2Iy2GIZqTa4",
        "outputId": "86f06ae9-c22d-4536-a768-0a7328670a87"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total GPT-2 vocab size: 50257\n",
            "\n",
            "Nonsense word 'zqxjklmno':\n",
            "  Token IDs      : [89, 80, 87, 73, 41582, 76, 3919]\n",
            "  Decoded        : 'zqxjklmno'\n",
            "  # subword units: 7\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q: How many subword units is it broken into?\n",
        "What does this say about how tiktoken handles unknown or rare words?\n",
        "\n",
        "\n",
        "A: This shows that unknown/rare words are split into multiple subwords,\n",
        "allowing the model to still represent and learn from them.\n"
      ],
      "metadata": {
        "id": "34u_hDJCq402"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) Token Batching and Padding (30 points)\n",
        "\n",
        "Using a list of short sentences:\n",
        "sentences = [\"Short.\", \"This one is longer.\", \"The longest sentence of all three.\"]\n",
        "- Tokenize each sentence.\n",
        "- Pad all sequences to match the length of the longest sequence (use 0 as the pad token).\n",
        "- Create an attention mask indicating real tokens (1) and padding (0).\n",
        "- Print: Padded token sequences, Attention masks"
      ],
      "metadata": {
        "id": "Yxog2s8bspvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences2 = [\n",
        "    \"Short.\",\n",
        "    \"This one is longer.\",\n",
        "    \"The longest sentence of all three.\"\n",
        "]\n",
        "\n",
        "# 1. Tokenize each\n",
        "tokenized = [tokenizer.encode(s) for s in sentences2]\n",
        "max_len = max(len(t) for t in tokenized)\n",
        "pad_id = 0  # GPT-2 uses 0 for the '<|pad|>' token in our scheme\n",
        "\n",
        "padded_seqs = []\n",
        "attention_masks = []\n",
        "\n",
        "for t in tokenized:\n",
        "    pad_length = max_len - len(t)\n",
        "    padded = t + [pad_id] * pad_length\n",
        "    mask   = [1] * len(t) + [0] * pad_length\n",
        "    padded_seqs.append(padded)\n",
        "    attention_masks.append(mask)\n",
        "\n",
        "for i, (p, m) in enumerate(zip(padded_seqs, attention_masks), 1):\n",
        "    decoded = tokenizer.decode([tok for tok in p if tok != pad_id])\n",
        "    print(f\"Sentence {i!r}:\")\n",
        "    print(f\"  Padded IDs    : {p}\")\n",
        "    print(f\"  Attention mask: {m}\")\n",
        "    print(f\"  Decoded (no pad): {decoded!r}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsDFnWQ_0_YH",
        "outputId": "bca2fdbe-f1d9-4c9a-d3ba-b0ca8fa91b00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1:\n",
            "  Padded IDs    : [16438, 13, 0, 0, 0, 0, 0]\n",
            "  Attention mask: [1, 1, 0, 0, 0, 0, 0]\n",
            "  Decoded (no pad): 'Short.'\n",
            "\n",
            "Sentence 2:\n",
            "  Padded IDs    : [1212, 530, 318, 2392, 13, 0, 0]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 0, 0]\n",
            "  Decoded (no pad): 'This one is longer.'\n",
            "\n",
            "Sentence 3:\n",
            "  Padded IDs    : [464, 14069, 6827, 286, 477, 1115, 13]\n",
            "  Attention mask: [1, 1, 1, 1, 1, 1, 1]\n",
            "  Decoded (no pad): 'The longest sentence of all three.'\n",
            "\n"
          ]
        }
      ]
    }
  ]
}