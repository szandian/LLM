{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqNJH9u1zRcN",
        "outputId": "6c5cdd0f-c4e6-4d47-891a-3651fcb1db91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# In[1]:\n",
        "# =============================================================================\n",
        "# Setup and Imports\n",
        "# =============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel # For loading pretrained weights\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In[2]:\n",
        "# =============================================================================\n",
        "# Part 1: Core Architecture Implementation\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# a) er(\"mask\", torch.tril(torch.ones(config[\"context_length\"], config[\"context_length\"]))\n",
        "                                     .view(1, 1, config[\"context_length\"], config[\"context_length\"]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (C)\n",
        "\n",
        "        # 1. Calculate query, key, values for all heads in batch and move head forward\n",
        "        q, k, v  = self.c_attn(x).split(self.emb_dim, dim=2)\n",
        "        k = k.view(B, T, selImplement the MultiHeadAttention class\n",
        "# -----------------------------------------------------------------------------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Multi-Head Causal Self-Attention as described in the GPT-2 paper.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config[\"emb_dim\"] % config[\"n_heads\"] == 0\n",
        "        self.emb_dim = config[\"emb_dim\"]\n",
        "        self.n_heads = config[\"n_heads\"]\n",
        "        self.head_dim = self.emb_dim // self.n_heads\n",
        "\n",
        "        # Combined key, query, value projections\n",
        "        self.c_attn = nn.Linear(self.emb_dim, 3 * self.emb_dim, bias=config[\"qkv_bias\"])\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=config[\"qkv_bias\"])\n",
        "        # Regularization\n",
        "        self.attn_dropout = nn.Dropout(config[\"drop_rate\"])\n",
        "        self.resid_dropout = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "        # Causal mask to prevent attending to future tokens\n",
        "        # Using register_buffer so it's part of the model's state but not a parameter\n",
        "        self.register_bufff.n_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # 2. Causal self-attention\n",
        "        # (B, nh, T, hs) @ (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(k.size(-1)))\n",
        "        # Apply causal mask\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        # Apply softmax\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "\n",
        "        # 3. Get weighted values\n",
        "        y = att @ v # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        # 4. Re-assemble heads\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # (B, T, C)\n",
        "\n",
        "        # 5. Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# b) Implement the TransformerBlock class\n",
        "# -----------------------------------------------------------------------------\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single block of the transformer, including multi-head attention and a feed-forward network.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config[\"emb_dim\"])\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config[\"emb_dim\"])\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"]),\n",
        "            nn.Dropout(config[\"drop_rate\"])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Residual connection around the attention layer\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        # Residual connection around the MLP\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# c) Implement the GPTModel class\n",
        "# -----------------------------------------------------------------------------\n",
        "class GPTModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The full GPT-2 model architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            # Token and positional embeddings\n",
        "            wte = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"]),\n",
        "            wpe = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"]),\n",
        "            drop = nn.Dropout(config[\"drop_rate\"]),\n",
        "            # Stack of transformer blocks\n",
        "            h = nn.ModuleList([TransformerBlock(config) for _ in range(config[\"n_layers\"])]),\n",
        "            # Final layer norm\n",
        "            ln_f = nn.LayerNorm(config[\"emb_dim\"]),\n",
        "        ))\n",
        "        # Language model head\n",
        "        self.lm_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "        # Weight tying: lm_head weights are tied with token embedding weights\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config[\"context_length\"], f\"Cannot forward sequence of length {T}, max is {self.config['context_length']}\"\n",
        "\n",
        "        # 1. Get token and position embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0) # shape (1, T)\n",
        "        tok_emb = self.transformer.wte(idx) # (B, T, C)\n",
        "        pos_emb = self.transformer.wpe(pos) # (1, T, C)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        # 2. Pass through transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        # 3. Final layer norm\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        # 4. Language model head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        # 5. Calculate loss if targets are provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Reshape for cross-entropy loss\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "print(\"Part 1: Core architecture implemented.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76mFuxX10C3m",
        "outputId": "d97ab2fe-4840-4425-a130-c95eb805d75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 1: Core architecture implemented.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In[3]:\n",
        "# =============================================================================\n",
        "# Part 2: Prepare for Training\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# a) Instantiate your model using the GPT_CONFIG_124M\n",
        "# -----------------------------------------------------------------------------\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,      # Vocabulary size for GPT-2\n",
        "    \"context_length\": 256,   # Context window size (reduced from 1024 for manageability)\n",
        "    \"emb_dim\": 768,          # Embedding dimension\n",
        "    \"n_heads\": 12,           # Number of attention heads\n",
        "    \"n_layers\": 12,          # Number of transformer layers\n",
        "    \"drop_rate\": 0.1,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Use bias in QKV projections (standard for GPT-2)\n",
        "}\n",
        "\n",
        "model_from_scratch = GPTModel(GPT_CONFIG_124M)\n",
        "model_from_scratch.to(device)\n",
        "print(f\"Model instantiated with {sum(p.numel() for p in model_from_scratch.parameters())/1e6:.2f}M parameters.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# b) Choose a small corpus and tokenize it\n",
        "# -----------------------------------------------------------------------------\n",
        "# For this demo, we'll use a small snippet of Shakespeare's \"The Tempest\"\n",
        "# to keep training fast and demonstrate the model learns the style.\n",
        "text_corpus = \"\"\"\n",
        "PROSPERO:\n",
        "If thou more murmur'st, I will rend an oak\n",
        "And peg thee in his knotty entrails till\n",
        "Thou hast howl'd away twelve winters.\n",
        "\n",
        "ARIEL:\n",
        "Pardon, master;\n",
        "I will be correspondent to command\n",
        "And do my spriting gently.\n",
        "\n",
        "PROSPERO:\n",
        "Do so, and after two days\n",
        "I will discharge thee.\n",
        "\n",
        "ARIEL:\n",
        "That's my noble master!\n",
        "What shall I do? say what; what shall I do?\n",
        "\n",
        "PROSPERO:\n",
        "Go make thyself like a nymph o' the sea: be subject\n",
        "To no sight but thine and mine, invisible\n",
        "To every eyeball else. Go take this shape\n",
        "And hither come in't: go, hence with diligence!\n",
        "\"\"\"\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "tokenized_text = tokenizer.encode(text_corpus)\n",
        "print(f\"Corpus length: {len(text_corpus)} characters, {len(tokenized_text)} tokens.\")\n",
        "\n",
        "# Create a simple dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokens, context_length):\n",
        "        self.tokens = tokens\n",
        "        self.context_length = context_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.context_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Grab a chunk of tokens\n",
        "        chunk = self.tokens[idx:idx + self.context_length + 1]\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# Prepare DataLoader\n",
        "context_length = 64 # Use a smaller context length for training to speed things up\n",
        "batch_size = 4\n",
        "train_dataset = TextDataset(tokenized_text, context_length)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f\"DataLoader created with {len(train_loader)} batches.\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# c) Train your GPT model on the dataset\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nStarting training...\")\n",
        "optimizer = torch.optim.AdamW(model_from_scratch.parameters(), lr=1e-4)\n",
        "epochs = 100 # Train for more epochs on this tiny dataset\n",
        "log_interval = 10\n",
        "\n",
        "model_from_scratch.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, loss = model_from_scratch(x, y)\n",
        "\n",
        "        if loss is not None:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % log_interval == 0:\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Save the trained model weights\n",
        "output_dir = \"./trained_gpt\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "torch.save(model_from_scratch.state_dict(), os.path.join(output_dir, \"custom_gpt_model.pth\"))\n",
        "print(f\"Model weights saved to {output_dir}/custom_gpt_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsk4xM4a0HRG",
        "outputId": "306f1364-1dd9-497b-9b22-de68d55b5217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model instantiated with 123.85M parameters.\n",
            "Corpus length: 550 characters, 176 tokens.\n",
            "DataLoader created with 28 batches.\n",
            "\n",
            "Starting training...\n",
            "Epoch 10/100 | Average Loss: 4.0904\n",
            "Epoch 20/100 | Average Loss: 4.0252\n",
            "Epoch 30/100 | Average Loss: 4.0154\n",
            "Epoch 40/100 | Average Loss: 4.0202\n",
            "Epoch 50/100 | Average Loss: 3.9873\n",
            "Epoch 60/100 | Average Loss: 3.8683\n",
            "Epoch 70/100 | Average Loss: 2.9654\n",
            "Epoch 80/100 | Average Loss: 1.1839\n",
            "Epoch 90/100 | Average Loss: 0.6015\n",
            "Epoch 100/100 | Average Loss: 0.4516\n",
            "Training finished.\n",
            "Model weights saved to ./trained_gpt/custom_gpt_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In[4]:\n",
        "# =============================================================================\n",
        "# Part 3: Inference and Demonstration\n",
        "# =============================================================================\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# a) Implement the generation function\n",
        "# -----------------------------------------------------------------------------\n",
        "def generate(model, input_ids, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "    \"\"\"\n",
        "    Generates text by iteratively predicting the next token.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    context_length = model.config[\"context_length\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context if it exceeds the model's supported length\n",
        "            idx_cond = input_ids if input_ids.size(1) <= context_length else input_ids[:, -context_length:]\n",
        "\n",
        "            # Get logits for the next token\n",
        "            logits, _ = model(idx_cond)\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Optional: Top-k sampling\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Sample from the distribution\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append the new token to the sequence\n",
        "            input_ids = torch.cat((input_ids, next_token_id), dim=1)\n",
        "\n",
        "    return input_ids\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# b) Demonstrate your model by generating completions\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Generating with Custom-Trained Model ---\")\n",
        "prompts = [\"PROSPERO:\", \"ARIEL:\\nPardon, master;\"]\n",
        "\n",
        "for prompt_text in prompts:\n",
        "    input_ids = tokenizer.encode(prompt_text, allowed_special={'<|endoftext|>'})\n",
        "    input_tensor = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    print(f\"\\nPrompt: '{prompt_text}'\")\n",
        "\n",
        "    # Generate completion\n",
        "    output_tensor = generate(model_from_scratch, input_tensor, max_new_tokens=30, temperature=0.8, top_k=10)\n",
        "\n",
        "    # Decode and print\n",
        "    generated_text = tokenizer.decode(output_tensor[0].tolist())\n",
        "    print(\"Generated Completion:\")\n",
        "    print(generated_text)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# c) Save the trained model and configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "def save_model(model, config, file_path):\n",
        "    \"\"\"Saves model state_dict and config together.\"\"\"\n",
        "    state = {\n",
        "        \"config\": config,\n",
        "        \"state_dict\": model.state_dict()\n",
        "    }\n",
        "    torch.save(state, file_path)\n",
        "    print(f\"Model and config saved to {file_path}\")\n",
        "\n",
        "def load_model(file_path):\n",
        "    \"\"\"Loads model and config, then instantiates the model.\"\"\"\n",
        "    state = torch.load(file_path, map_location=device)\n",
        "    config = state[\"config\"]\n",
        "    model = GPTModel(config)\n",
        "    model.load_state_dict(state[\"state_dict\"])\n",
        "    model.to(device)\n",
        "    print(f\"Model loaded from {file_path}\")\n",
        "    return model, config\n",
        "\n",
        "# Demonstrate save/load\n",
        "save_path = os.path.join(output_dir, \"full_model_package.pth\")\n",
        "save_model(model_from_scratch, GPT_CONFIG_124M, save_path)\n",
        "loaded_model, loaded_config = load_model(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcIUDjqU0Thw",
        "outputId": "4cab0667-210b-4fd2-9785-609c1a2fd86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating with Custom-Trained Model ---\n",
            "\n",
            "Prompt: 'PROSPERO:'\n",
            "Generated Completion:\n",
            "PROSPERO:\n",
            "Do shall I do? say what; what shall I do?\n",
            "\n",
            "PROSPERO:\n",
            "Go make thyself like a nymph\n",
            "------------------------------\n",
            "\n",
            "Prompt: 'ARIEL:\n",
            "Pardon, master;'\n",
            "Generated Completion:\n",
            "ARIEL:\n",
            "Pardon, master;\n",
            "\n",
            "ARIEL:\n",
            "That's my noble master!\n",
            "What shall I do? say what; what shall I do?\n",
            "\n",
            "PROS\n",
            "------------------------------\n",
            "Model and config saved to ./trained_gpt/full_model_package.pth\n",
            "Model loaded from ./trained_gpt/full_model_package.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLTDC_cn0qew"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}